{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMjwq6pS-kFz"
      },
      "source": [
        "# Stock NeurIPS2018 Part 2. Train\n",
        "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*.\n",
        "\n",
        "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
        "\n",
        "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT-zXutMgqOS"
      },
      "source": [
        "# Part 1. Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D0vEcPxSJ8hI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce919112-bea0-46bc-f1b0-150ea17e87d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: websockets==15.0.1 in /usr/local/lib/python3.11/dist-packages (15.0.1)\n",
            "Collecting git+https://github.com/andwir/FinRL-Mods.git\n",
            "  Cloning https://github.com/andwir/FinRL-Mods.git to /tmp/pip-req-build-cr89s_l7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andwir/FinRL-Mods.git /tmp/pip-req-build-cr89s_l7\n",
            "  Resolved https://github.com/andwir/FinRL-Mods.git to commit 9f9905ed84c7b8d97a86ecf66ae3c6aa85ab8c61\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git (from finrl==0.3.8)\n",
            "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /tmp/pip-install-qi3ypqsl/elegantrl_6b92fa5ef9e04c34ae5ca78b55b83361\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /tmp/pip-install-qi3ypqsl/elegantrl_6b92fa5ef9e04c34ae5ca78b55b83361\n",
            "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit 8ea76afc3e7f1564ae9f0e69e70254116d575fe9\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting alpaca-py<0.38,>=0.37 (from finrl==0.3.8)\n",
            "  Downloading alpaca_py-0.37.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting alpaca-trade-api<4,>=3 (from finrl==0.3.8)\n",
            "  Downloading alpaca_trade_api-3.2.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting ccxt<4,>=3 (from finrl==0.3.8)\n",
            "  Downloading ccxt-3.1.60-py2.py3-none-any.whl.metadata (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.7/108.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jqdatasdk<2,>=1 (from finrl==0.3.8)\n",
            "  Downloading jqdatasdk-1.9.7-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting pyfolio-reloaded<0.10,>=0.9 (from finrl==0.3.8)\n",
            "  Downloading pyfolio_reloaded-0.9.8-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting pyportfolioopt<2,>=1 (from finrl==0.3.8)\n",
            "  Downloading pyportfolioopt-1.5.6-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting ray<3,>=2 (from ray[default,tune]<3,>=2->finrl==0.3.8)\n",
            "  Downloading ray-2.46.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: scikit-learn<2,>=1 in /usr/local/lib/python3.11/dist-packages (from finrl==0.3.8) (1.6.1)\n",
            "Collecting selenium<5,>=4 (from finrl==0.3.8)\n",
            "  Downloading selenium-4.32.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting stable-baselines3>=2.0.0a5 (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8)\n",
            "  Downloading stable_baselines3-2.6.1a1-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting stockstats<0.6,>=0.5 (from finrl==0.3.8)\n",
            "  Downloading stockstats-0.5.4-py2.py3-none-any.whl.metadata (26 kB)\n",
            "Collecting webdriver-manager<5,>=4 (from finrl==0.3.8)\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting wrds<4,>=3 (from finrl==0.3.8)\n",
            "  Downloading wrds-3.3.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: yfinance<0.3,>=0.2 in /usr/local/lib/python3.11/dist-packages (from finrl==0.3.8) (0.2.61)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.1.0)\n",
            "Requirement already satisfied: pandas>=1.5.3 in /usr/local/lib/python3.11/dist-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.2.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.11.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.30.0 in /usr/local/lib/python3.11/dist-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.32.3)\n",
            "Collecting sseclient-py<2.0.0,>=1.7.2 (from alpaca-py<0.38,>=0.37->finrl==0.3.8)\n",
            "  Downloading sseclient_py-1.8.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (15.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (2.0.2)\n",
            "Collecting urllib3<2,>1.24 (from alpaca-trade-api<4,>=3->finrl==0.3.8)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.8.0)\n",
            "Collecting websockets>=10.4 (from alpaca-py<0.38,>=0.37->finrl==0.3.8)\n",
            "  Downloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting msgpack<2.0.0,>=1.0.3 (from alpaca-py<0.38,>=0.37->finrl==0.3.8)\n",
            "  Downloading msgpack-1.0.3.tar.gz (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (3.11.15)\n",
            "Collecting PyYAML==6.0.1 (from alpaca-trade-api<4,>=3->finrl==0.3.8)\n",
            "  Downloading PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting deprecation==2.1.0 (from alpaca-trade-api<4,>=3->finrl==0.3.8)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.8) (24.2)\n",
            "Requirement already satisfied: setuptools>=60.9.0 in /usr/local/lib/python3.11/dist-packages (from ccxt<4,>=3->finrl==0.3.8) (75.2.0)\n",
            "Requirement already satisfied: certifi>=2018.1.18 in /usr/local/lib/python3.11/dist-packages (from ccxt<4,>=3->finrl==0.3.8) (2025.4.26)\n",
            "Requirement already satisfied: cryptography>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from ccxt<4,>=3->finrl==0.3.8) (43.0.3)\n",
            "Collecting aiodns>=1.1.1 (from ccxt<4,>=3->finrl==0.3.8)\n",
            "  Downloading aiodns-3.4.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: yarl>=1.7.2 in /usr/local/lib/python3.11/dist-packages (from ccxt<4,>=3->finrl==0.3.8) (1.20.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.17.0)\n",
            "Requirement already satisfied: SQLAlchemy>=1.2.8 in /usr/local/lib/python3.11/dist-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (2.0.40)\n",
            "Collecting pymysql>=0.7.6 (from jqdatasdk<2,>=1->finrl==0.3.8)\n",
            "  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting thriftpy2!=0.5.1,>=0.3.9 (from jqdatasdk<2,>=1->finrl==0.3.8)\n",
            "  Downloading thriftpy2-0.5.2.tar.gz (782 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.3/782.3 kB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ipython>=3.2.3 in /usr/local/lib/python3.11/dist-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (7.34.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.10.0)\n",
            "Requirement already satisfied: pytz>=2014.10 in /usr/local/lib/python3.11/dist-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2025.2)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.15.3)\n",
            "Requirement already satisfied: seaborn>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.13.2)\n",
            "Collecting empyrical-reloaded>=0.5.9 (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8)\n",
            "  Downloading empyrical_reloaded-0.5.11-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: cvxpy>=1.1.19 in /usr/local/lib/python3.11/dist-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (1.6.5)\n",
            "Collecting ecos<3.0.0,>=2.0.14 (from pyportfolioopt<2,>=1->finrl==0.3.8)\n",
            "  Downloading ecos-2.0.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: plotly<6.0.0,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (5.24.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (8.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (3.18.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (4.23.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (5.29.4)\n",
            "Collecting aiohttp_cors (from ray[default,tune]<3,>=2->finrl==0.3.8)\n",
            "  Downloading aiohttp_cors-0.8.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting colorful (from ray[default,tune]<3,>=2->finrl==0.3.8)\n",
            "  Downloading colorful-0.5.6-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Collecting py-spy>=0.2.0 (from ray[default,tune]<3,>=2->finrl==0.3.8)\n",
            "  Downloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (1.71.0)\n",
            "Collecting opencensus (from ray[default,tune]<3,>=2->finrl==0.3.8)\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: prometheus_client>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.21.1)\n",
            "Requirement already satisfied: smart_open in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (7.1.0)\n",
            "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[default,tune]<3,>=2->finrl==0.3.8)\n",
            "  Downloading virtualenv-20.31.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting tensorboardX>=1.9 (from ray[default,tune]<3,>=2->finrl==0.3.8)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pyarrow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (18.1.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2025.3.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2,>=1->finrl==0.3.8) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2,>=1->finrl==0.3.8) (3.6.0)\n",
            "Collecting trio~=0.17 (from selenium<5,>=4->finrl==0.3.8)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium<5,>=4->finrl==0.3.8)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium<5,>=4->finrl==0.3.8) (4.13.2)\n",
            "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.1.1)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.0+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.1.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.11.0.86)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.18.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (13.9.4)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.11.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (11.2.1)\n",
            "Collecting python-dotenv (from webdriver-manager<5,>=4->finrl==0.3.8)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting psycopg2-binary<2.10,>=2.9 (from wrds<4,>=3->finrl==0.3.8)\n",
            "  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (4.3.8)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (3.18.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (4.13.4)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (0.10.0)\n",
            "INFO: pip is looking at multiple versions of yfinance to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting yfinance<0.3,>=0.2 (from finrl==0.3.8)\n",
            "  Downloading yfinance-0.2.60-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "  Downloading yfinance-0.2.59-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "  Downloading yfinance-0.2.58-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting th (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8)\n",
            "  Downloading th-0.4.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting pycares>=4.0.0 (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.8)\n",
            "  Downloading pycares-4.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (0.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.8) (2.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (1.17.1)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (1.0.4)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (0.10.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.11/dist-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (3.2.7.post2)\n",
            "Requirement already satisfied: bottleneck>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from empyrical-reloaded>=0.5.9->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.4.2)\n",
            "Collecting peewee>=3.16.2 (from yfinance<0.3,>=0.2->finrl==0.3.8)\n",
            "  Downloading peewee-3.17.3.tar.gz (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.0.4)\n",
            "Collecting jedi>=0.16 (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly<6.0.0,>=5.0.0->pyportfolioopt<2,>=1->finrl==0.3.8) (9.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.10)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.2.8->jqdatasdk<2,>=1->finrl==0.3.8) (3.2.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.1.3)\n",
            "Requirement already satisfied: Cython>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.0.12)\n",
            "Requirement already satisfied: ply<4.0,>=3.4 in /usr/local/lib/python3.11/dist-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (2.4.0)\n",
            "Collecting outcome (from trio~=0.17->selenium<5,>=4->finrl==0.3.8)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium<5,>=4->finrl==0.3.8) (1.7.1)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.8)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.24.0)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8)\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.24.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart_open->ray[default,tune]<3,>=2->finrl==0.3.8) (1.17.2)\n",
            "Collecting niltype<2.0,>=0.3 (from th->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8)\n",
            "  Downloading niltype-1.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (2.22)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.70.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.26.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.38.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.8.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.1.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.2)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (0.16.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (4.9.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.6.1)\n",
            "Downloading alpaca_py-0.37.0-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alpaca_trade_api-3.2.0-py3-none-any.whl (34 kB)\n",
            "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.7/757.7 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ccxt-3.1.60-py2.py3-none-any.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jqdatasdk-1.9.7-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyfolio_reloaded-0.9.8-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyportfolioopt-1.5.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.46.0-cp311-cp311-manylinux2014_x86_64.whl (68.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading selenium-4.32.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m121.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stable_baselines3-2.6.1a1-py3-none-any.whl (185 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.3/185.3 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stockstats-0.5.4-py2.py3-none-any.whl (21 kB)\n",
            "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading wrds-3.3.0-py3-none-any.whl (13 kB)\n",
            "Downloading yfinance-0.2.58-py2.py3-none-any.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.7/113.7 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiodns-3.4.0-py3-none-any.whl (7.1 kB)\n",
            "Downloading ecos-2.0.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (220 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.1/220.1 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading empyrical_reloaded-0.5.11-py3-none-any.whl (32 kB)\n",
            "Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sseclient_py-1.8.0-py2.py3-none-any.whl (8.8 kB)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading virtualenv-20.31.2-py3-none-any.whl (6.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m121.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_cors-0.8.1-py3-none-any.whl (25 kB)\n",
            "Downloading colorful-0.5.6-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading th-0.4.1-py3-none-any.whl (12 kB)\n",
            "Downloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading niltype-1.0.2-py3-none-any.whl (5.3 kB)\n",
            "Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading pycares-4.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (626 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.2/626.2 kB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: finrl, msgpack, elegantrl, peewee, thriftpy2\n",
            "  Building wheel for finrl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for finrl: filename=finrl-0.3.8-py3-none-any.whl size=4707853 sha256=3a1ff820e915c8a010d8410aa4da1f43b99931f0206be209815810f33d07c8c0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wu_h7qih/wheels/4e/19/ef/e9cc35fea5f830c12dcc7cdc0ddc99b226d6a5a2f2934745bd\n",
            "  Building wheel for msgpack (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for msgpack: filename=msgpack-1.0.3-cp311-cp311-linux_x86_64.whl size=15688 sha256=653a098f93a398c57264396a744eba7d8b14e973c7ee98aa8b1def960a6325b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/35/da/ed9b26b510235e00e3a3c3bab7bad97b59214729662255ab3d\n",
            "  Building wheel for elegantrl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for elegantrl: filename=ElegantRL-0.3.10-py3-none-any.whl size=272023 sha256=27c5dfdf4fd2528430ffe641a4c8b3e084fec69bba6d1dc635d88195ba38cc80\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wu_h7qih/wheels/9a/77/4d/6284111037b2dd64af9ef18d4d600d9c185cc2f6f09704e896\n",
            "  Building wheel for peewee (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for peewee: filename=peewee-3.17.3-cp311-cp311-linux_x86_64.whl size=763059 sha256=eb687658531db6b36a00ee91768d79d14786b2f77ff94f18ebcbcfb93a001f66\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/3d/30/a54ad8c2307aa653f234a6f651ab12fbc5cfbb3f383145ab6a\n",
            "  Building wheel for thriftpy2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for thriftpy2: filename=thriftpy2-0.5.2-cp311-cp311-linux_x86_64.whl size=1836687 sha256=303d5572ae659b2835da916edb74e8ca6ca2a8ecdf2a9b94e13bda809a870766\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/32/fa/51ae0364792430fb80858f4705c9e0cba3d6900f591f0c4495\n",
            "Successfully built finrl msgpack elegantrl peewee thriftpy2\n",
            "Installing collected packages: sseclient-py, py-spy, peewee, opencensus-context, msgpack, distlib, colorful, wsproto, websockets, virtualenv, urllib3, thriftpy2, tensorboardX, PyYAML, python-dotenv, pymysql, psycopg2-binary, outcome, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, niltype, jedi, deprecation, trio, th, pycares, nvidia-cusparse-cu12, nvidia-cudnn-cu12, ecos, yfinance, wrds, webdriver-manager, trio-websocket, stockstats, nvidia-cusolver-cu12, jqdatasdk, empyrical-reloaded, elegantrl, alpaca-trade-api, alpaca-py, aiohttp_cors, aiodns, selenium, ray, pyportfolioopt, pyfolio-reloaded, opencensus, ccxt, stable-baselines3, finrl\n",
            "  Attempting uninstall: peewee\n",
            "    Found existing installation: peewee 3.18.1\n",
            "    Uninstalling peewee-3.18.1:\n",
            "      Successfully uninstalled peewee-3.18.1\n",
            "  Attempting uninstall: msgpack\n",
            "    Found existing installation: msgpack 1.1.0\n",
            "    Uninstalling msgpack-1.1.0:\n",
            "      Successfully uninstalled msgpack-1.1.0\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.4.0\n",
            "    Uninstalling urllib3-2.4.0:\n",
            "      Successfully uninstalled urllib3-2.4.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: yfinance\n",
            "    Found existing installation: yfinance 0.2.61\n",
            "    Uninstalling yfinance-0.2.61:\n",
            "      Successfully uninstalled yfinance-0.2.61\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 0.7.3 requires websockets>=14.0, but you have websockets 10.4 which is incompatible.\n",
            "google-genai 1.15.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyYAML-6.0.1 aiodns-3.4.0 aiohttp_cors-0.8.1 alpaca-py-0.37.0 alpaca-trade-api-3.2.0 ccxt-3.1.60 colorful-0.5.6 deprecation-2.1.0 distlib-0.3.9 ecos-2.0.14 elegantrl-0.3.10 empyrical-reloaded-0.5.11 finrl-0.3.8 jedi-0.19.2 jqdatasdk-1.9.7 msgpack-1.0.3 niltype-1.0.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 opencensus-0.11.4 opencensus-context-0.1.3 outcome-1.3.0.post0 peewee-3.17.3 psycopg2-binary-2.9.10 py-spy-0.4.0 pycares-4.8.0 pyfolio-reloaded-0.9.8 pymysql-1.1.1 pyportfolioopt-1.5.6 python-dotenv-1.1.0 ray-2.46.0 selenium-4.32.0 sseclient-py-1.8.0 stable-baselines3-2.6.1a1 stockstats-0.5.4 tensorboardX-2.6.2.2 th-0.4.1 thriftpy2-0.5.2 trio-0.30.0 trio-websocket-0.12.2 urllib3-1.26.20 virtualenv-20.31.2 webdriver-manager-4.0.2 websockets-10.4 wrds-3.3.0 wsproto-1.2.0 yfinance-0.2.58\n"
          ]
        }
      ],
      "source": [
        "## install finrl library\n",
        "!pip install websockets==15.0.1\n",
        "!pip install git+https://github.com/andwir/FinRL-Mods.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install pandas_market_calendars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiMPL5M85iKt",
        "outputId": "56c6e16c-6daf-4d1f-fd84-05cb9995d63f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas_market_calendars\n",
            "  Downloading pandas_market_calendars-5.1.0-py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: pandas>=1.1 in /usr/local/lib/python3.11/dist-packages (from pandas_market_calendars) (2.2.2)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from pandas_market_calendars) (2025.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from pandas_market_calendars) (2.9.0.post0)\n",
            "Collecting exchange-calendars>=3.3 (from pandas_market_calendars)\n",
            "  Downloading exchange_calendars-4.10.1-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from exchange-calendars>=3.3->pandas_market_calendars) (2.0.2)\n",
            "Collecting pyluach (from exchange-calendars>=3.3->pandas_market_calendars)\n",
            "  Downloading pyluach-2.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.11/dist-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.12.1)\n",
            "Collecting korean_lunar_calendar (from exchange-calendars>=3.3->pandas_market_calendars)\n",
            "  Downloading korean_lunar_calendar-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1->pandas_market_calendars) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->pandas_market_calendars) (1.17.0)\n",
            "Downloading pandas_market_calendars-5.1.0-py3-none-any.whl (123 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.9/123.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading exchange_calendars-4.10.1-py3-none-any.whl (200 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading korean_lunar_calendar-0.3.1-py3-none-any.whl (9.0 kB)\n",
            "Downloading pyluach-2.2.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: korean_lunar_calendar, pyluach, exchange-calendars, pandas_market_calendars\n",
            "Successfully installed exchange-calendars-4.10.1 korean_lunar_calendar-0.3.1 pandas_market_calendars-5.1.0 pyluach-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xt1317y2ixSS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from stable_baselines3.common.logger import configure\n",
        "\n",
        "from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "\n",
        "check_and_make_directories([TRAINED_MODEL_DIR])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWrSrQv3i0Ng"
      },
      "source": [
        "# Part 2. Build A Market Environment in OpenAI Gym-style"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiHhM2U-XBMZ"
      },
      "source": [
        "![rl_diagram_transparent_bg.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeneTRdyZDvy"
      },
      "source": [
        "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process:\n",
        "\n",
        "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
        "\n",
        "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3H88JXkI93v"
      },
      "source": [
        "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
        "\n",
        "state-action-reward are specified as follows:\n",
        "\n",
        "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
        "\n",
        "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
        "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
        "\n",
        "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
        "\n",
        "\n",
        "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKyZejI0fmp1"
      },
      "source": [
        "## Read data\n",
        "\n",
        "We first read the .csv file of our training data into dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mFCP1YEhi6oi"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('train_data.csv')\n",
        "# If you are not using the data generated from part 1 of this tutorial, make sure\n",
        "# it has the columns and index in the form that could be make into the environment.\n",
        "# Then you can comment and skip the following two lines.\n",
        "train = train.set_index(train.columns[0])\n",
        "train.index.names = ['']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw95ZMicgEyi"
      },
      "source": [
        "## Construct the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WZ6-9q2gq9S"
      },
      "source": [
        "Calculate and specify the parameters we need for constructing the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T3DZPoaIm8k",
        "outputId": "183ba499-fc8b-421b-f55d-4fde738d4b98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stock Dimension: 29, State Space: 291, len(INDICATORS): 8\n"
          ]
        }
      ],
      "source": [
        "stock_dimension = len(train.tic.unique())\n",
        "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}, len(INDICATORS): {len(INDICATORS)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WsOLoeNcJF8Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43cac530-78fc-4077-969e-44bf134ce944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]: [0] num_stock_shares:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "buy_cost_list = sell_cost_list = [0.01] * stock_dimension\n",
        "num_stock_shares = [0] * stock_dimension\n",
        "print(f\"[0]: {[0]} num_stock_shares:{num_stock_shares}\")\n",
        "\n",
        "env_kwargs = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"num_stock_shares\": num_stock_shares,\n",
        "    \"buy_cost_pct\": buy_cost_list,\n",
        "    \"sell_cost_pct\": sell_cost_list,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4\n",
        "}\n",
        "\n",
        "\n",
        "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7We-q73jjaFQ"
      },
      "source": [
        "## Environment for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS-SHiGRJK-4",
        "outputId": "930cb8ae-faef-4790-eef1-dbd6411ab47b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
          ]
        }
      ],
      "source": [
        "env_train, _ = e_train_gym.get_sb_env()\n",
        "print(type(env_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMNR5nHjh1iz"
      },
      "source": [
        "# Part 3: Train DRL Agents\n",
        "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
        "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "364PsqckttcQ"
      },
      "outputs": [],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "\n",
        "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
        "if_using_a2c = True\n",
        "if_using_ddpg = True\n",
        "if_using_ppo = True\n",
        "if_using_td3 = True\n",
        "if_using_sac = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDmqOyF9h1iz"
      },
      "source": [
        "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uijiWgkuh1jB"
      },
      "source": [
        "### Agent 1: A2C\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUCnkn-HIbmj",
        "outputId": "f5b9983d-823c-4b8e-83da-76241cc71025"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
            "Using cuda device\n",
            "Logging to results/a2c\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_a2c = agent.get_model(\"a2c\")\n",
        "\n",
        "if if_using_a2c:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/a2c'\n",
        "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_a2c.set_logger(new_logger_a2c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GVpkWGqH4-D",
        "outputId": "b96a9314-4c3b-40b4-ecca-4a07f39ac87e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 86        |\n",
            "|    iterations         | 100       |\n",
            "|    time_elapsed       | 5         |\n",
            "|    total_timesteps    | 500       |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | -0.0242   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 99        |\n",
            "|    policy_loss        | -77.7     |\n",
            "|    reward             | 0.1567285 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 5.49      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 87         |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | -111       |\n",
            "|    reward             | -1.1507146 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 8.27       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 93       |\n",
            "|    iterations         | 300      |\n",
            "|    time_elapsed       | 16       |\n",
            "|    total_timesteps    | 1500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 299      |\n",
            "|    policy_loss        | -103     |\n",
            "|    reward             | 5.759383 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 7.6      |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 93         |\n",
            "|    iterations         | 400        |\n",
            "|    time_elapsed       | 21         |\n",
            "|    total_timesteps    | 2000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 399        |\n",
            "|    policy_loss        | -40.1      |\n",
            "|    reward             | -1.0459876 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 14.7       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 95        |\n",
            "|    iterations         | 500       |\n",
            "|    time_elapsed       | 26        |\n",
            "|    total_timesteps    | 2500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 499       |\n",
            "|    policy_loss        | 429       |\n",
            "|    reward             | -8.657084 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 157       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 96         |\n",
            "|    iterations         | 600        |\n",
            "|    time_elapsed       | 31         |\n",
            "|    total_timesteps    | 3000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | -0.889     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 599        |\n",
            "|    policy_loss        | 277        |\n",
            "|    reward             | 0.13133875 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 46.6       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 94         |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 36         |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | -0.0323    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -78.4      |\n",
            "|    reward             | -3.4340608 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 4.7        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 96         |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 41         |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | -0.0284    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | -18.2      |\n",
            "|    reward             | -4.1517196 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.35       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 95        |\n",
            "|    iterations         | 900       |\n",
            "|    time_elapsed       | 47        |\n",
            "|    total_timesteps    | 4500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 899       |\n",
            "|    policy_loss        | 74.4      |\n",
            "|    reward             | -2.365012 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 4.42      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 96        |\n",
            "|    iterations         | 1000      |\n",
            "|    time_elapsed       | 51        |\n",
            "|    total_timesteps    | 5000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | -0.0298   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 999       |\n",
            "|    policy_loss        | -28       |\n",
            "|    reward             | -3.885523 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.49      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 96       |\n",
            "|    iterations         | 1100     |\n",
            "|    time_elapsed       | 56       |\n",
            "|    total_timesteps    | 5500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1099     |\n",
            "|    policy_loss        | -494     |\n",
            "|    reward             | 6.144372 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 168      |\n",
            "------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 96           |\n",
            "|    iterations         | 1200         |\n",
            "|    time_elapsed       | 62           |\n",
            "|    total_timesteps    | 6000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -41.4        |\n",
            "|    explained_variance | -0.0203      |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 1199         |\n",
            "|    policy_loss        | -98          |\n",
            "|    reward             | -0.120754726 |\n",
            "|    std                | 1.01         |\n",
            "|    value_loss         | 6.21         |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 97        |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 66        |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | -51.7     |\n",
            "|    reward             | -4.001733 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 4.44      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 96        |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 72        |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 156       |\n",
            "|    reward             | 1.4606265 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 24.6      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 97        |\n",
            "|    iterations         | 1500      |\n",
            "|    time_elapsed       | 77        |\n",
            "|    total_timesteps    | 7500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1499      |\n",
            "|    policy_loss        | 238       |\n",
            "|    reward             | 1.4485712 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 31        |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 96         |\n",
            "|    iterations         | 1600       |\n",
            "|    time_elapsed       | 82         |\n",
            "|    total_timesteps    | 8000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1599       |\n",
            "|    policy_loss        | -62.4      |\n",
            "|    reward             | 0.53180194 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 10.4       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 97       |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 87       |\n",
            "|    total_timesteps    | 8500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | -208     |\n",
            "|    reward             | 4.136872 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 56.4     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 97        |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 92        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | -0.0682   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | -18.4     |\n",
            "|    reward             | 1.6482769 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.357     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 97        |\n",
            "|    iterations         | 1900      |\n",
            "|    time_elapsed       | 97        |\n",
            "|    total_timesteps    | 9500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1899      |\n",
            "|    policy_loss        | 45.4      |\n",
            "|    reward             | 1.1602712 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.05      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 97          |\n",
            "|    iterations         | 2000        |\n",
            "|    time_elapsed       | 102         |\n",
            "|    total_timesteps    | 10000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.6       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1999        |\n",
            "|    policy_loss        | -117        |\n",
            "|    reward             | 0.088846736 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 10.9        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 97       |\n",
            "|    iterations         | 2100     |\n",
            "|    time_elapsed       | 108      |\n",
            "|    total_timesteps    | 10500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.6    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2099     |\n",
            "|    policy_loss        | -13.4    |\n",
            "|    reward             | 4.361372 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 7.93     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 97        |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 112       |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | -13.7     |\n",
            "|    reward             | -9.333798 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 10.3      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 97        |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 117       |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | -3.14e+03 |\n",
            "|    reward             | 1.5472871 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 5.5e+03   |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 97        |\n",
            "|    iterations         | 2400      |\n",
            "|    time_elapsed       | 122       |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | 24.2      |\n",
            "|    reward             | 0.7544194 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 3.51      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 97          |\n",
            "|    iterations         | 2500        |\n",
            "|    time_elapsed       | 127         |\n",
            "|    total_timesteps    | 12500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.7       |\n",
            "|    explained_variance | -0.00602    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2499        |\n",
            "|    policy_loss        | -96.3       |\n",
            "|    reward             | -0.52905726 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 5.95        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 97       |\n",
            "|    iterations         | 2600     |\n",
            "|    time_elapsed       | 133      |\n",
            "|    total_timesteps    | 13000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2599     |\n",
            "|    policy_loss        | -38.5    |\n",
            "|    reward             | 0.427256 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 1.34     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 97        |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 137       |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | 0.005     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | -11.4     |\n",
            "|    reward             | -0.451797 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.256     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 97        |\n",
            "|    iterations         | 2800      |\n",
            "|    time_elapsed       | 143       |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | 0.105     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | 179       |\n",
            "|    reward             | 2.6104484 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 20.5      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 97        |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 148       |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | -74.1     |\n",
            "|    reward             | 2.0814621 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 3.42      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 3000       |\n",
            "|    time_elapsed       | 152        |\n",
            "|    total_timesteps    | 15000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.8      |\n",
            "|    explained_variance | 0.0355     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2999       |\n",
            "|    policy_loss        | 11.1       |\n",
            "|    reward             | 0.46158844 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 0.361      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 97        |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 158       |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | -32.4     |\n",
            "|    reward             | -1.239237 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 2.91      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 3200       |\n",
            "|    time_elapsed       | 163        |\n",
            "|    total_timesteps    | 16000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.9      |\n",
            "|    explained_variance | -0.127     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3199       |\n",
            "|    policy_loss        | -129       |\n",
            "|    reward             | -1.4250743 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 22.5       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 97       |\n",
            "|    iterations         | 3300     |\n",
            "|    time_elapsed       | 168      |\n",
            "|    total_timesteps    | 16500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42      |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3299     |\n",
            "|    policy_loss        | 104      |\n",
            "|    reward             | 3.310247 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 9.9      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 173       |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42       |\n",
            "|    explained_variance | 0.156     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | 293       |\n",
            "|    reward             | 2.2698965 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 54.7      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 3500       |\n",
            "|    time_elapsed       | 178        |\n",
            "|    total_timesteps    | 17500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3499       |\n",
            "|    policy_loss        | 112        |\n",
            "|    reward             | -1.1172854 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 11.3       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 3600       |\n",
            "|    time_elapsed       | 183        |\n",
            "|    total_timesteps    | 18000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3599       |\n",
            "|    policy_loss        | -67.5      |\n",
            "|    reward             | -0.6548098 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 5.28       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 188       |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | 90.3      |\n",
            "|    reward             | 2.5337126 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 6.82      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 193       |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42       |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | 92.2      |\n",
            "|    reward             | 0.1684155 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 5.23      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 198       |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42       |\n",
            "|    explained_variance | -1.58     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | -22.6     |\n",
            "|    reward             | 0.5905764 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 1.15      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 97        |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 204       |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | -96.4     |\n",
            "|    reward             | 2.7510133 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 6.13      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 4100       |\n",
            "|    time_elapsed       | 208        |\n",
            "|    total_timesteps    | 20500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.1      |\n",
            "|    explained_variance | 0.0174     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4099       |\n",
            "|    policy_loss        | -34.8      |\n",
            "|    reward             | 0.06116468 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 1.61       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 4200      |\n",
            "|    time_elapsed       | 213       |\n",
            "|    total_timesteps    | 21000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4199      |\n",
            "|    policy_loss        | -46.9     |\n",
            "|    reward             | 0.9664849 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 2.21      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 98       |\n",
            "|    iterations         | 4300     |\n",
            "|    time_elapsed       | 219      |\n",
            "|    total_timesteps    | 21500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4299     |\n",
            "|    policy_loss        | -18.8    |\n",
            "|    reward             | 4.34956  |\n",
            "|    std                | 1.04     |\n",
            "|    value_loss         | 1.27     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 98       |\n",
            "|    iterations         | 4400     |\n",
            "|    time_elapsed       | 223      |\n",
            "|    total_timesteps    | 22000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4399     |\n",
            "|    policy_loss        | 118      |\n",
            "|    reward             | 2.036691 |\n",
            "|    std                | 1.04     |\n",
            "|    value_loss         | 14.7     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 4500       |\n",
            "|    time_elapsed       | 229        |\n",
            "|    total_timesteps    | 22500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4499       |\n",
            "|    policy_loss        | 153        |\n",
            "|    reward             | -0.5406373 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 18.7       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 234       |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | 104       |\n",
            "|    reward             | 1.3827896 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 6.75      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 98          |\n",
            "|    iterations         | 4700        |\n",
            "|    time_elapsed       | 239         |\n",
            "|    total_timesteps    | 23500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.3       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4699        |\n",
            "|    policy_loss        | -99.5       |\n",
            "|    reward             | 0.044653144 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 8.68        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 4800       |\n",
            "|    time_elapsed       | 244        |\n",
            "|    total_timesteps    | 24000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4799       |\n",
            "|    policy_loss        | -159       |\n",
            "|    reward             | -0.7939798 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 12.4       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 249       |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | -36.9     |\n",
            "|    reward             | 1.0027413 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 1.82      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 98          |\n",
            "|    iterations         | 5000        |\n",
            "|    time_elapsed       | 254         |\n",
            "|    total_timesteps    | 25000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.3       |\n",
            "|    explained_variance | 0.00107     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4999        |\n",
            "|    policy_loss        | 23.6        |\n",
            "|    reward             | -0.39743665 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 1.98        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 5100       |\n",
            "|    time_elapsed       | 259        |\n",
            "|    total_timesteps    | 25500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5099       |\n",
            "|    policy_loss        | -111       |\n",
            "|    reward             | 0.24303335 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 13.9       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 98       |\n",
            "|    iterations         | 5200     |\n",
            "|    time_elapsed       | 264      |\n",
            "|    total_timesteps    | 26000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5199     |\n",
            "|    policy_loss        | -447     |\n",
            "|    reward             | 4.195399 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 124      |\n",
            "------------------------------------\n",
            "day: 2892, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2757582.97\n",
            "total_reward: 1757582.97\n",
            "total_cost: 423181.31\n",
            "total_trades: 51575\n",
            "Sharpe: 0.609\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 5300       |\n",
            "|    time_elapsed       | 269        |\n",
            "|    total_timesteps    | 26500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | -2.67      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5299       |\n",
            "|    policy_loss        | 33.4       |\n",
            "|    reward             | 0.43007013 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 0.873      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 274       |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | -372      |\n",
            "|    reward             | 1.2098774 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 83.3      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 98       |\n",
            "|    iterations         | 5500     |\n",
            "|    time_elapsed       | 280      |\n",
            "|    total_timesteps    | 27500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.4    |\n",
            "|    explained_variance | -0.0635  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5499     |\n",
            "|    policy_loss        | -32.3    |\n",
            "|    reward             | 1.813423 |\n",
            "|    std                | 1.04     |\n",
            "|    value_loss         | 6.2      |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 5600       |\n",
            "|    time_elapsed       | 284        |\n",
            "|    total_timesteps    | 28000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | 0.228      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5599       |\n",
            "|    policy_loss        | -134       |\n",
            "|    reward             | -0.3269642 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 11.2       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 5700       |\n",
            "|    time_elapsed       | 290        |\n",
            "|    total_timesteps    | 28500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5699       |\n",
            "|    policy_loss        | -328       |\n",
            "|    reward             | -3.3956852 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 79.7       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 295       |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.4     |\n",
            "|    explained_variance | -13.7     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | 204       |\n",
            "|    reward             | 2.4574666 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 91.1      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 5900       |\n",
            "|    time_elapsed       | 300        |\n",
            "|    total_timesteps    | 29500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5899       |\n",
            "|    policy_loss        | -12.8      |\n",
            "|    reward             | 0.35512903 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 0.432      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 6000      |\n",
            "|    time_elapsed       | 305       |\n",
            "|    total_timesteps    | 30000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.4     |\n",
            "|    explained_variance | 0.0428    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5999      |\n",
            "|    policy_loss        | 4.93      |\n",
            "|    reward             | 0.4180568 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 0.452     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 6100       |\n",
            "|    time_elapsed       | 310        |\n",
            "|    total_timesteps    | 30500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6099       |\n",
            "|    policy_loss        | -27.1      |\n",
            "|    reward             | -2.6093566 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 1.1        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 6200      |\n",
            "|    time_elapsed       | 315       |\n",
            "|    total_timesteps    | 31000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.5     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6199      |\n",
            "|    policy_loss        | -123      |\n",
            "|    reward             | -2.149792 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 9.32      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 98       |\n",
            "|    iterations         | 6300     |\n",
            "|    time_elapsed       | 320      |\n",
            "|    total_timesteps    | 31500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6299     |\n",
            "|    policy_loss        | 109      |\n",
            "|    reward             | 1.707182 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 8.71     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 6400      |\n",
            "|    time_elapsed       | 326       |\n",
            "|    total_timesteps    | 32000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6399      |\n",
            "|    policy_loss        | 150       |\n",
            "|    reward             | 1.7848068 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 15.9      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 6500       |\n",
            "|    time_elapsed       | 330        |\n",
            "|    total_timesteps    | 32500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.5      |\n",
            "|    explained_variance | -0.191     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6499       |\n",
            "|    policy_loss        | -25.8      |\n",
            "|    reward             | -2.3950632 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 4.06       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 98          |\n",
            "|    iterations         | 6600        |\n",
            "|    time_elapsed       | 335         |\n",
            "|    total_timesteps    | 33000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.5       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6599        |\n",
            "|    policy_loss        | -89.1       |\n",
            "|    reward             | -0.19992769 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 7.87        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 6700       |\n",
            "|    time_elapsed       | 341        |\n",
            "|    total_timesteps    | 33500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | -0.0441    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6699       |\n",
            "|    policy_loss        | -603       |\n",
            "|    reward             | -2.9888685 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 257        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 6800       |\n",
            "|    time_elapsed       | 345        |\n",
            "|    total_timesteps    | 34000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6799       |\n",
            "|    policy_loss        | -144       |\n",
            "|    reward             | 0.73510814 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 12.3       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 98          |\n",
            "|    iterations         | 6900        |\n",
            "|    time_elapsed       | 351         |\n",
            "|    total_timesteps    | 34500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6899        |\n",
            "|    policy_loss        | 240         |\n",
            "|    reward             | -0.44509146 |\n",
            "|    std                | 1.05        |\n",
            "|    value_loss         | 42.6        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 7000       |\n",
            "|    time_elapsed       | 356        |\n",
            "|    total_timesteps    | 35000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6999       |\n",
            "|    policy_loss        | 29.1       |\n",
            "|    reward             | 0.39455676 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 0.871      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 7100       |\n",
            "|    time_elapsed       | 361        |\n",
            "|    total_timesteps    | 35500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7099       |\n",
            "|    policy_loss        | 12.4       |\n",
            "|    reward             | 0.45697334 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 0.375      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 7200       |\n",
            "|    time_elapsed       | 366        |\n",
            "|    total_timesteps    | 36000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.5      |\n",
            "|    explained_variance | 0.0732     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7199       |\n",
            "|    policy_loss        | -228       |\n",
            "|    reward             | 0.74622625 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 28.7       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 7300       |\n",
            "|    time_elapsed       | 371        |\n",
            "|    total_timesteps    | 36500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7299       |\n",
            "|    policy_loss        | -51.1      |\n",
            "|    reward             | -2.3150742 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 4.76       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 98       |\n",
            "|    iterations         | 7400     |\n",
            "|    time_elapsed       | 376      |\n",
            "|    total_timesteps    | 37000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7399     |\n",
            "|    policy_loss        | 197      |\n",
            "|    reward             | -5.32564 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 29.5     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 98       |\n",
            "|    iterations         | 7500     |\n",
            "|    time_elapsed       | 381      |\n",
            "|    total_timesteps    | 37500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 7499     |\n",
            "|    policy_loss        | -18.9    |\n",
            "|    reward             | 2.942698 |\n",
            "|    std                | 1.05     |\n",
            "|    value_loss         | 1.58     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 7600      |\n",
            "|    time_elapsed       | 386       |\n",
            "|    total_timesteps    | 38000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7599      |\n",
            "|    policy_loss        | 10        |\n",
            "|    reward             | 1.6002488 |\n",
            "|    std                | 1.05      |\n",
            "|    value_loss         | 1.12      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 7700       |\n",
            "|    time_elapsed       | 391        |\n",
            "|    total_timesteps    | 38500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.7      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7699       |\n",
            "|    policy_loss        | -47.6      |\n",
            "|    reward             | 0.91252387 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 1.7        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 7800      |\n",
            "|    time_elapsed       | 396       |\n",
            "|    total_timesteps    | 39000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7799      |\n",
            "|    policy_loss        | 5.77      |\n",
            "|    reward             | 0.1841298 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 1.74      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 7900      |\n",
            "|    time_elapsed       | 401       |\n",
            "|    total_timesteps    | 39500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.8     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7899      |\n",
            "|    policy_loss        | 171       |\n",
            "|    reward             | 1.8792862 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 19.3      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 8000      |\n",
            "|    time_elapsed       | 406       |\n",
            "|    total_timesteps    | 40000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7999      |\n",
            "|    policy_loss        | 92        |\n",
            "|    reward             | 1.3474928 |\n",
            "|    std                | 1.06      |\n",
            "|    value_loss         | 9.47      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 98       |\n",
            "|    iterations         | 8100     |\n",
            "|    time_elapsed       | 412      |\n",
            "|    total_timesteps    | 40500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 8099     |\n",
            "|    policy_loss        | 84.9     |\n",
            "|    reward             | 5.144508 |\n",
            "|    std                | 1.06     |\n",
            "|    value_loss         | 8.92     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 8200       |\n",
            "|    time_elapsed       | 416        |\n",
            "|    total_timesteps    | 41000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8199       |\n",
            "|    policy_loss        | 23.2       |\n",
            "|    reward             | 0.11642024 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 0.473      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 8300       |\n",
            "|    time_elapsed       | 422        |\n",
            "|    total_timesteps    | 41500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.9      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8299       |\n",
            "|    policy_loss        | -99.6      |\n",
            "|    reward             | -2.3616314 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 6.15       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 98          |\n",
            "|    iterations         | 8400        |\n",
            "|    time_elapsed       | 426         |\n",
            "|    total_timesteps    | 42000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.9       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8399        |\n",
            "|    policy_loss        | -89.6       |\n",
            "|    reward             | -0.20106946 |\n",
            "|    std                | 1.07        |\n",
            "|    value_loss         | 8.85        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 8500       |\n",
            "|    time_elapsed       | 431        |\n",
            "|    total_timesteps    | 42500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8499       |\n",
            "|    policy_loss        | -108       |\n",
            "|    reward             | -1.0496941 |\n",
            "|    std                | 1.06       |\n",
            "|    value_loss         | 10.2       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 8600      |\n",
            "|    time_elapsed       | 437       |\n",
            "|    total_timesteps    | 43000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8599      |\n",
            "|    policy_loss        | 129       |\n",
            "|    reward             | -4.917744 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 20        |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 8700       |\n",
            "|    time_elapsed       | 441        |\n",
            "|    total_timesteps    | 43500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8699       |\n",
            "|    policy_loss        | -15.7      |\n",
            "|    reward             | 0.22086188 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 1.34       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 8800      |\n",
            "|    time_elapsed       | 447       |\n",
            "|    total_timesteps    | 44000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43       |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8799      |\n",
            "|    policy_loss        | -25.2     |\n",
            "|    reward             | 0.6797971 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 0.653     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 8900      |\n",
            "|    time_elapsed       | 452       |\n",
            "|    total_timesteps    | 44500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8899      |\n",
            "|    policy_loss        | -9.05     |\n",
            "|    reward             | 0.2477988 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 1.4       |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 98          |\n",
            "|    iterations         | 9000        |\n",
            "|    time_elapsed       | 457         |\n",
            "|    total_timesteps    | 45000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -43.1       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8999        |\n",
            "|    policy_loss        | 0.0179      |\n",
            "|    reward             | -0.34688574 |\n",
            "|    std                | 1.07        |\n",
            "|    value_loss         | 0.754       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 9100       |\n",
            "|    time_elapsed       | 462        |\n",
            "|    total_timesteps    | 45500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.1      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9099       |\n",
            "|    policy_loss        | -11.3      |\n",
            "|    reward             | 0.49371788 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 0.333      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 9200      |\n",
            "|    time_elapsed       | 466       |\n",
            "|    total_timesteps    | 46000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9199      |\n",
            "|    policy_loss        | 63.2      |\n",
            "|    reward             | 4.7090893 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 5.45      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 9300      |\n",
            "|    time_elapsed       | 472       |\n",
            "|    total_timesteps    | 46500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.1     |\n",
            "|    explained_variance | -0.0171   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9299      |\n",
            "|    policy_loss        | -68.6     |\n",
            "|    reward             | 1.1700191 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 3.68      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 98          |\n",
            "|    iterations         | 9400        |\n",
            "|    time_elapsed       | 477         |\n",
            "|    total_timesteps    | 47000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -43.1       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9399        |\n",
            "|    policy_loss        | 140         |\n",
            "|    reward             | 0.076262146 |\n",
            "|    std                | 1.07        |\n",
            "|    value_loss         | 13.1        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 9500       |\n",
            "|    time_elapsed       | 482        |\n",
            "|    total_timesteps    | 47500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9499       |\n",
            "|    policy_loss        | 55.8       |\n",
            "|    reward             | 0.15558042 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 2.67       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 9600       |\n",
            "|    time_elapsed       | 487        |\n",
            "|    total_timesteps    | 48000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9599       |\n",
            "|    policy_loss        | -219       |\n",
            "|    reward             | -1.2216249 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 27.6       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 9700      |\n",
            "|    time_elapsed       | 492       |\n",
            "|    total_timesteps    | 48500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9699      |\n",
            "|    policy_loss        | 96.8      |\n",
            "|    reward             | -0.150007 |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 6.34      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 9800      |\n",
            "|    time_elapsed       | 497       |\n",
            "|    total_timesteps    | 49000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -43.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9799      |\n",
            "|    policy_loss        | 15.9      |\n",
            "|    reward             | 2.195325  |\n",
            "|    std                | 1.07      |\n",
            "|    value_loss         | 27.2      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 98          |\n",
            "|    iterations         | 9900        |\n",
            "|    time_elapsed       | 502         |\n",
            "|    total_timesteps    | 49500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -43.1       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9899        |\n",
            "|    policy_loss        | 54.7        |\n",
            "|    reward             | 0.062628865 |\n",
            "|    std                | 1.07        |\n",
            "|    value_loss         | 1.45        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 98         |\n",
            "|    iterations         | 10000      |\n",
            "|    time_elapsed       | 507        |\n",
            "|    total_timesteps    | 50000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -43.1      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9999       |\n",
            "|    policy_loss        | 33.6       |\n",
            "|    reward             | -0.2636235 |\n",
            "|    std                | 1.07       |\n",
            "|    value_loss         | 1.58       |\n",
            "--------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_a2c = agent.train_model(model=model_a2c,\n",
        "                             tb_log_name='a2c',\n",
        "                             total_timesteps=50000) if if_using_a2c else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zjCWfgsg3sVa"
      },
      "outputs": [],
      "source": [
        "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRiOtrywfAo1"
      },
      "source": [
        "### Agent 2: DDPG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "M2YadjfnLwgt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e25cb81e-2966-4c72-e17f-aa3cf0396baa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
            "Using cuda device\n",
            "Logging to results/ddpg\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_ddpg = agent.get_model(\"ddpg\")\n",
        "\n",
        "if if_using_ddpg:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ddpg'\n",
        "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ddpg.set_logger(new_logger_ddpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tCDa78rqfO_a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1128c33b-d23e-435b-c53a-4b65aa53f820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "day: 2892, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5031268.44\n",
            "total_reward: 4031268.44\n",
            "total_cost: 48894.08\n",
            "total_trades: 50315\n",
            "Sharpe: 0.931\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 66        |\n",
            "|    time_elapsed    | 172       |\n",
            "|    total_timesteps | 11572     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -624      |\n",
            "|    critic_loss     | 1e+03     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 11471     |\n",
            "|    reward          | 3.9063754 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 66        |\n",
            "|    time_elapsed    | 346       |\n",
            "|    total_timesteps | 23144     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -394      |\n",
            "|    critic_loss     | 13.8      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 23043     |\n",
            "|    reward          | 3.9063754 |\n",
            "----------------------------------\n",
            "day: 2892, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4958873.51\n",
            "total_reward: 3958873.51\n",
            "total_cost: 9900.98\n",
            "total_trades: 49138\n",
            "Sharpe: 0.877\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 65        |\n",
            "|    time_elapsed    | 527       |\n",
            "|    total_timesteps | 34716     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -233      |\n",
            "|    critic_loss     | 10.3      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 34615     |\n",
            "|    reward          | 3.9063754 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 65        |\n",
            "|    time_elapsed    | 705       |\n",
            "|    total_timesteps | 46288     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -141      |\n",
            "|    critic_loss     | 11.5      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 46187     |\n",
            "|    reward          | 3.9063754 |\n",
            "----------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_ddpg = agent.train_model(model=model_ddpg,\n",
        "                             tb_log_name='ddpg',\n",
        "                             total_timesteps=50000) if if_using_ddpg else None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cBx0X9JsPSur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ne6M2R-WvrUQ"
      },
      "outputs": [],
      "source": [
        "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gDkU-j-fCmZ"
      },
      "source": [
        "### Agent 3: PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "y5D5PFUhMzSV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e377e22b-0c20-43c7-9ce0-a37a006d7a0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cuda device\n",
            "Logging to results/ppo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "PPO_PARAMS = {\n",
        "    \"n_steps\": 2048,\n",
        "    \"ent_coef\": 0.01,\n",
        "    \"learning_rate\": 0.00025,\n",
        "    \"batch_size\": 128,\n",
        "}\n",
        "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
        "\n",
        "if if_using_ppo:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ppo'\n",
        "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ppo.set_logger(new_logger_ppo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Gt8eIQKYM4G3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3619a300-1b49-4f78-b5e0-8682aa4431c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------\n",
            "| time/              |              |\n",
            "|    fps             | 97           |\n",
            "|    iterations      | 1            |\n",
            "|    time_elapsed    | 20           |\n",
            "|    total_timesteps | 2048         |\n",
            "| train/             |              |\n",
            "|    reward          | -0.060280338 |\n",
            "-------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 99         |\n",
            "|    iterations           | 2          |\n",
            "|    time_elapsed         | 41         |\n",
            "|    total_timesteps      | 4096       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.0148851  |\n",
            "|    clip_fraction        | 0.203      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.2      |\n",
            "|    explained_variance   | -0.0996    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 1.11       |\n",
            "|    n_updates            | 10         |\n",
            "|    policy_gradient_loss | -0.0345    |\n",
            "|    reward               | 0.30801508 |\n",
            "|    std                  | 1          |\n",
            "|    value_loss           | 3.25       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 101         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 60          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020223105 |\n",
            "|    clip_fraction        | 0.244       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.013      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.513       |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0279     |\n",
            "|    reward               | -1.4320095  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 2.77        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 101         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 80          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020929722 |\n",
            "|    clip_fraction        | 0.255       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | 0.189       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.575       |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0307     |\n",
            "|    reward               | 0.12817445  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 1.91        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 14325.24\n",
            "total_reward: -985674.76\n",
            "total_cost: 1943853.04\n",
            "total_trades: 72652\n",
            "Sharpe: -1.974\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 101         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 100         |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017128225 |\n",
            "|    clip_fraction        | 0.161       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | 0.0296      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.579       |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0282     |\n",
            "|    reward               | 1.018261    |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 2.48        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 102         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 119         |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018819204 |\n",
            "|    clip_fraction        | 0.193       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | 0.112       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.22        |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0209     |\n",
            "|    reward               | 1.1882119   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 3.26        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 102         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 140         |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025402974 |\n",
            "|    clip_fraction        | 0.251       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | 0.177       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.498       |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.0238     |\n",
            "|    reward               | 0.043306105 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 3.16        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 102         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 159         |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021167804 |\n",
            "|    clip_fraction        | 0.242       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | 0.309       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.442       |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.026      |\n",
            "|    reward               | 0.15441707  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 2.14        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 102         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 179         |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016388949 |\n",
            "|    clip_fraction        | 0.148       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.5       |\n",
            "|    explained_variance   | 0.00157     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.22        |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0201     |\n",
            "|    reward               | 0.30505785  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 5.41        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 102         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 199         |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021401903 |\n",
            "|    clip_fraction        | 0.227       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.5       |\n",
            "|    explained_variance   | 0.331       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.896       |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.0328     |\n",
            "|    reward               | 0.50520635  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 3.61        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 218         |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024160167 |\n",
            "|    clip_fraction        | 0.265       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.5       |\n",
            "|    explained_variance   | 0.425       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.409       |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.0251     |\n",
            "|    reward               | 0.047800913 |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 1.95        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 238         |\n",
            "|    total_timesteps      | 24576       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017635718 |\n",
            "|    clip_fraction        | 0.178       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.6       |\n",
            "|    explained_variance   | 0.157       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.45        |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.0227     |\n",
            "|    reward               | -0.148897   |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 4.45        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 257         |\n",
            "|    total_timesteps      | 26624       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020629127 |\n",
            "|    clip_fraction        | 0.192       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.6       |\n",
            "|    explained_variance   | 0.573       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.59        |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.0223     |\n",
            "|    reward               | 0.41119504  |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 3.84        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 103          |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 277          |\n",
            "|    total_timesteps      | 28672        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.019423284  |\n",
            "|    clip_fraction        | 0.195        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -41.6        |\n",
            "|    explained_variance   | 0.296        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.74         |\n",
            "|    n_updates            | 130          |\n",
            "|    policy_gradient_loss | -0.0266      |\n",
            "|    reward               | -0.012548735 |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 4.92         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 297         |\n",
            "|    total_timesteps      | 30720       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017762076 |\n",
            "|    clip_fraction        | 0.179       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.6       |\n",
            "|    explained_variance   | 0.421       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 0.6         |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.0232     |\n",
            "|    reward               | 2.060235    |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 2.58        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 103        |\n",
            "|    iterations           | 16         |\n",
            "|    time_elapsed         | 316        |\n",
            "|    total_timesteps      | 32768      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01825232 |\n",
            "|    clip_fraction        | 0.183      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.7      |\n",
            "|    explained_variance   | 0.151      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 2.36       |\n",
            "|    n_updates            | 150        |\n",
            "|    policy_gradient_loss | -0.0178    |\n",
            "|    reward               | 0.05115423 |\n",
            "|    std                  | 1.02       |\n",
            "|    value_loss           | 5.45       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 336         |\n",
            "|    total_timesteps      | 34816       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025038991 |\n",
            "|    clip_fraction        | 0.199       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.7       |\n",
            "|    explained_variance   | 0.514       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.49        |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.0229     |\n",
            "|    reward               | 0.38721743  |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 4.96        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 103          |\n",
            "|    iterations           | 18           |\n",
            "|    time_elapsed         | 356          |\n",
            "|    total_timesteps      | 36864        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.024862658  |\n",
            "|    clip_fraction        | 0.249        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -41.7        |\n",
            "|    explained_variance   | 0.532        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 1.34         |\n",
            "|    n_updates            | 170          |\n",
            "|    policy_gradient_loss | -0.0249      |\n",
            "|    reward               | -0.065953985 |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 3.27         |\n",
            "------------------------------------------\n",
            "day: 2892, episode: 50\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 341836.06\n",
            "total_reward: -658163.94\n",
            "total_cost: 2418063.21\n",
            "total_trades: 75676\n",
            "Sharpe: -0.434\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 375         |\n",
            "|    total_timesteps      | 38912       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017772708 |\n",
            "|    clip_fraction        | 0.191       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.7       |\n",
            "|    explained_variance   | 0.44        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.89        |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | -0.0166     |\n",
            "|    reward               | -0.45058176 |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 6.13        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 103        |\n",
            "|    iterations           | 20         |\n",
            "|    time_elapsed         | 395        |\n",
            "|    total_timesteps      | 40960      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03129935 |\n",
            "|    clip_fraction        | 0.267      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.8      |\n",
            "|    explained_variance   | 0.614      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 1.92       |\n",
            "|    n_updates            | 190        |\n",
            "|    policy_gradient_loss | -0.0191    |\n",
            "|    reward               | -0.4514398 |\n",
            "|    std                  | 1.02       |\n",
            "|    value_loss           | 5.47       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 415         |\n",
            "|    total_timesteps      | 43008       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.039714776 |\n",
            "|    clip_fraction        | 0.327       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.8       |\n",
            "|    explained_variance   | 0.65        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 1.08        |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.0211     |\n",
            "|    reward               | -2.7344742  |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 3.12        |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 103           |\n",
            "|    iterations           | 22            |\n",
            "|    time_elapsed         | 434           |\n",
            "|    total_timesteps      | 45056         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.016071899   |\n",
            "|    clip_fraction        | 0.187         |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -41.9         |\n",
            "|    explained_variance   | 0.197         |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 5             |\n",
            "|    n_updates            | 210           |\n",
            "|    policy_gradient_loss | -0.0156       |\n",
            "|    reward               | 0.00032938304 |\n",
            "|    std                  | 1.02          |\n",
            "|    value_loss           | 12.5          |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 23          |\n",
            "|    time_elapsed         | 455         |\n",
            "|    total_timesteps      | 47104       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022489391 |\n",
            "|    clip_fraction        | 0.221       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.9       |\n",
            "|    explained_variance   | 0.421       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.7         |\n",
            "|    n_updates            | 220         |\n",
            "|    policy_gradient_loss | -0.0212     |\n",
            "|    reward               | 0.1363348   |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 12.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 474         |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019563831 |\n",
            "|    clip_fraction        | 0.205       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.9       |\n",
            "|    explained_variance   | 0.555       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.63        |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.0205     |\n",
            "|    reward               | 1.8509316   |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 9.31        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 494         |\n",
            "|    total_timesteps      | 51200       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015685044 |\n",
            "|    clip_fraction        | 0.164       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.9       |\n",
            "|    explained_variance   | 0.466       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.18        |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.0215     |\n",
            "|    reward               | -0.4495849  |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 15.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 26          |\n",
            "|    time_elapsed         | 514         |\n",
            "|    total_timesteps      | 53248       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021061491 |\n",
            "|    clip_fraction        | 0.204       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.9       |\n",
            "|    explained_variance   | 0.56        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.74        |\n",
            "|    n_updates            | 250         |\n",
            "|    policy_gradient_loss | -0.0207     |\n",
            "|    reward               | 0.7722799   |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 7.16        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 533         |\n",
            "|    total_timesteps      | 55296       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.03573603  |\n",
            "|    clip_fraction        | 0.341       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.9       |\n",
            "|    explained_variance   | 0.82        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 2.14        |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | -0.0233     |\n",
            "|    reward               | -0.40536094 |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 5.22        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 553         |\n",
            "|    total_timesteps      | 57344       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022355173 |\n",
            "|    clip_fraction        | 0.194       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42         |\n",
            "|    explained_variance   | 0.576       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.87        |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | -0.0199     |\n",
            "|    reward               | -1.0059509  |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 10.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 29          |\n",
            "|    time_elapsed         | 572         |\n",
            "|    total_timesteps      | 59392       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.02513789  |\n",
            "|    clip_fraction        | 0.281       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42         |\n",
            "|    explained_variance   | 0.182       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 9.18        |\n",
            "|    n_updates            | 280         |\n",
            "|    policy_gradient_loss | -0.0158     |\n",
            "|    reward               | -0.15535712 |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 15.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 30          |\n",
            "|    time_elapsed         | 592         |\n",
            "|    total_timesteps      | 61440       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021821178 |\n",
            "|    clip_fraction        | 0.228       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.1       |\n",
            "|    explained_variance   | 0.359       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.35        |\n",
            "|    n_updates            | 290         |\n",
            "|    policy_gradient_loss | -0.0165     |\n",
            "|    reward               | 0.7920941   |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 12.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 31          |\n",
            "|    time_elapsed         | 612         |\n",
            "|    total_timesteps      | 63488       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021048073 |\n",
            "|    clip_fraction        | 0.275       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.1       |\n",
            "|    explained_variance   | 0.00102     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 12.2        |\n",
            "|    n_updates            | 300         |\n",
            "|    policy_gradient_loss | -0.0069     |\n",
            "|    reward               | 1.1701002   |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 31          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 32          |\n",
            "|    time_elapsed         | 631         |\n",
            "|    total_timesteps      | 65536       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016532227 |\n",
            "|    clip_fraction        | 0.182       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.2       |\n",
            "|    explained_variance   | 0.124       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 9.84        |\n",
            "|    n_updates            | 310         |\n",
            "|    policy_gradient_loss | -0.0198     |\n",
            "|    reward               | 0.3327261   |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 16.1        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 60\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1123532.95\n",
            "total_reward: 123532.95\n",
            "total_cost: 2505986.25\n",
            "total_trades: 75408\n",
            "Sharpe: 0.150\n",
            "=================================\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 103        |\n",
            "|    iterations           | 33         |\n",
            "|    time_elapsed         | 651        |\n",
            "|    total_timesteps      | 67584      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02299343 |\n",
            "|    clip_fraction        | 0.29       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.2      |\n",
            "|    explained_variance   | 0.261      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 8.93       |\n",
            "|    n_updates            | 320        |\n",
            "|    policy_gradient_loss | -0.0201    |\n",
            "|    reward               | 0.1213997  |\n",
            "|    std                  | 1.04       |\n",
            "|    value_loss           | 17.6       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 34          |\n",
            "|    time_elapsed         | 671         |\n",
            "|    total_timesteps      | 69632       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022312969 |\n",
            "|    clip_fraction        | 0.239       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.3       |\n",
            "|    explained_variance   | 0.631       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.01        |\n",
            "|    n_updates            | 330         |\n",
            "|    policy_gradient_loss | -0.0244     |\n",
            "|    reward               | 0.805651    |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 11          |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 103        |\n",
            "|    iterations           | 35         |\n",
            "|    time_elapsed         | 690        |\n",
            "|    total_timesteps      | 71680      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02946125 |\n",
            "|    clip_fraction        | 0.234      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.3      |\n",
            "|    explained_variance   | 0.612      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 4          |\n",
            "|    n_updates            | 340        |\n",
            "|    policy_gradient_loss | -0.017     |\n",
            "|    reward               | -0.6653459 |\n",
            "|    std                  | 1.04       |\n",
            "|    value_loss           | 8.84       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 36          |\n",
            "|    time_elapsed         | 710         |\n",
            "|    total_timesteps      | 73728       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031405456 |\n",
            "|    clip_fraction        | 0.259       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.4       |\n",
            "|    explained_variance   | 0.3         |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.43        |\n",
            "|    n_updates            | 350         |\n",
            "|    policy_gradient_loss | -0.012      |\n",
            "|    reward               | -2.995778   |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 10.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 37          |\n",
            "|    time_elapsed         | 729         |\n",
            "|    total_timesteps      | 75776       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.03468199  |\n",
            "|    clip_fraction        | 0.297       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.4       |\n",
            "|    explained_variance   | 0.58        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.29        |\n",
            "|    n_updates            | 360         |\n",
            "|    policy_gradient_loss | -0.0158     |\n",
            "|    reward               | -0.24674515 |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 15.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 38          |\n",
            "|    time_elapsed         | 749         |\n",
            "|    total_timesteps      | 77824       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021565821 |\n",
            "|    clip_fraction        | 0.214       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.4       |\n",
            "|    explained_variance   | 0.1         |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 15.9        |\n",
            "|    n_updates            | 370         |\n",
            "|    policy_gradient_loss | -0.00893    |\n",
            "|    reward               | -5.7401533  |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 49          |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 103        |\n",
            "|    iterations           | 39         |\n",
            "|    time_elapsed         | 769        |\n",
            "|    total_timesteps      | 79872      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03056116 |\n",
            "|    clip_fraction        | 0.265      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.5      |\n",
            "|    explained_variance   | 0.12       |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 6.94       |\n",
            "|    n_updates            | 380        |\n",
            "|    policy_gradient_loss | -0.00981   |\n",
            "|    reward               | -1.455571  |\n",
            "|    std                  | 1.05       |\n",
            "|    value_loss           | 16.9       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 40          |\n",
            "|    time_elapsed         | 788         |\n",
            "|    total_timesteps      | 81920       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022602174 |\n",
            "|    clip_fraction        | 0.22        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.5       |\n",
            "|    explained_variance   | 0.272       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.2         |\n",
            "|    n_updates            | 390         |\n",
            "|    policy_gradient_loss | -0.0121     |\n",
            "|    reward               | -0.30937174 |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 18.6        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 103        |\n",
            "|    iterations           | 41         |\n",
            "|    time_elapsed         | 808        |\n",
            "|    total_timesteps      | 83968      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02663209 |\n",
            "|    clip_fraction        | 0.263      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.6      |\n",
            "|    explained_variance   | 0.46       |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 7.97       |\n",
            "|    n_updates            | 400        |\n",
            "|    policy_gradient_loss | -0.0106    |\n",
            "|    reward               | 0.37553555 |\n",
            "|    std                  | 1.05       |\n",
            "|    value_loss           | 19.7       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 42          |\n",
            "|    time_elapsed         | 827         |\n",
            "|    total_timesteps      | 86016       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.03440495  |\n",
            "|    clip_fraction        | 0.313       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.6       |\n",
            "|    explained_variance   | 0.174       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.14        |\n",
            "|    n_updates            | 410         |\n",
            "|    policy_gradient_loss | -0.00215    |\n",
            "|    reward               | -0.73638344 |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 23.2        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 103        |\n",
            "|    iterations           | 43         |\n",
            "|    time_elapsed         | 847        |\n",
            "|    total_timesteps      | 88064      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02802759 |\n",
            "|    clip_fraction        | 0.249      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.6      |\n",
            "|    explained_variance   | 0.302      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 4.32       |\n",
            "|    n_updates            | 420        |\n",
            "|    policy_gradient_loss | -0.0144    |\n",
            "|    reward               | 1.6638232  |\n",
            "|    std                  | 1.05       |\n",
            "|    value_loss           | 10.1       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 44          |\n",
            "|    time_elapsed         | 867         |\n",
            "|    total_timesteps      | 90112       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030327786 |\n",
            "|    clip_fraction        | 0.241       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.6       |\n",
            "|    explained_variance   | 0.445       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 9.2         |\n",
            "|    n_updates            | 430         |\n",
            "|    policy_gradient_loss | -0.0177     |\n",
            "|    reward               | -0.06420085 |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 18.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 45          |\n",
            "|    time_elapsed         | 886         |\n",
            "|    total_timesteps      | 92160       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017468072 |\n",
            "|    clip_fraction        | 0.186       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.6       |\n",
            "|    explained_variance   | 0.234       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 12.2        |\n",
            "|    n_updates            | 440         |\n",
            "|    policy_gradient_loss | -0.0109     |\n",
            "|    reward               | 0.65715003  |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 28.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 46          |\n",
            "|    time_elapsed         | 906         |\n",
            "|    total_timesteps      | 94208       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024229836 |\n",
            "|    clip_fraction        | 0.231       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.7       |\n",
            "|    explained_variance   | 0.245       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 10.1        |\n",
            "|    n_updates            | 450         |\n",
            "|    policy_gradient_loss | -0.0154     |\n",
            "|    reward               | -5.8000093  |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 19.2        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 70\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1111365.16\n",
            "total_reward: 111365.16\n",
            "total_cost: 2392402.21\n",
            "total_trades: 73964\n",
            "Sharpe: 0.146\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 47          |\n",
            "|    time_elapsed         | 926         |\n",
            "|    total_timesteps      | 96256       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022974819 |\n",
            "|    clip_fraction        | 0.221       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.7       |\n",
            "|    explained_variance   | 0.375       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.44        |\n",
            "|    n_updates            | 460         |\n",
            "|    policy_gradient_loss | -0.0183     |\n",
            "|    reward               | 0.97746843  |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 18.2        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 103        |\n",
            "|    iterations           | 48         |\n",
            "|    time_elapsed         | 946        |\n",
            "|    total_timesteps      | 98304      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03468684 |\n",
            "|    clip_fraction        | 0.298      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.8      |\n",
            "|    explained_variance   | 0.522      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 7.66       |\n",
            "|    n_updates            | 470        |\n",
            "|    policy_gradient_loss | -0.0161    |\n",
            "|    reward               | 13.376093  |\n",
            "|    std                  | 1.06       |\n",
            "|    value_loss           | 15         |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 49          |\n",
            "|    time_elapsed         | 966         |\n",
            "|    total_timesteps      | 100352      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.032078054 |\n",
            "|    clip_fraction        | 0.286       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.9       |\n",
            "|    explained_variance   | 0.156       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 14          |\n",
            "|    n_updates            | 480         |\n",
            "|    policy_gradient_loss | -0.0147     |\n",
            "|    reward               | -0.86996055 |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 31          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 50          |\n",
            "|    time_elapsed         | 987         |\n",
            "|    total_timesteps      | 102400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030411206 |\n",
            "|    clip_fraction        | 0.335       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.9       |\n",
            "|    explained_variance   | 0.285       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.61        |\n",
            "|    n_updates            | 490         |\n",
            "|    policy_gradient_loss | -0.0158     |\n",
            "|    reward               | -1.3425695  |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 13.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 51          |\n",
            "|    time_elapsed         | 1006        |\n",
            "|    total_timesteps      | 104448      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026057689 |\n",
            "|    clip_fraction        | 0.282       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43         |\n",
            "|    explained_variance   | 0.343       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.95        |\n",
            "|    n_updates            | 500         |\n",
            "|    policy_gradient_loss | -0.0132     |\n",
            "|    reward               | -0.34460464 |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 20.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 52          |\n",
            "|    time_elapsed         | 1027        |\n",
            "|    total_timesteps      | 106496      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022772476 |\n",
            "|    clip_fraction        | 0.209       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43         |\n",
            "|    explained_variance   | 0.439       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.33        |\n",
            "|    n_updates            | 510         |\n",
            "|    policy_gradient_loss | -0.0249     |\n",
            "|    reward               | -0.72700006 |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 18.2        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 103        |\n",
            "|    iterations           | 53         |\n",
            "|    time_elapsed         | 1047       |\n",
            "|    total_timesteps      | 108544     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03575646 |\n",
            "|    clip_fraction        | 0.308      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43        |\n",
            "|    explained_variance   | 0.422      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 3.59       |\n",
            "|    n_updates            | 520        |\n",
            "|    policy_gradient_loss | -0.0192    |\n",
            "|    reward               | 0.75039613 |\n",
            "|    std                  | 1.07       |\n",
            "|    value_loss           | 7.91       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 54          |\n",
            "|    time_elapsed         | 1067        |\n",
            "|    total_timesteps      | 110592      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.035867155 |\n",
            "|    clip_fraction        | 0.304       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.1       |\n",
            "|    explained_variance   | 0.545       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.95        |\n",
            "|    n_updates            | 530         |\n",
            "|    policy_gradient_loss | -0.013      |\n",
            "|    reward               | 1.8836664   |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 15.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 55          |\n",
            "|    time_elapsed         | 1087        |\n",
            "|    total_timesteps      | 112640      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025256615 |\n",
            "|    clip_fraction        | 0.286       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.1       |\n",
            "|    explained_variance   | 0.525       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.29        |\n",
            "|    n_updates            | 540         |\n",
            "|    policy_gradient_loss | -0.00888    |\n",
            "|    reward               | 0.05773559  |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 15.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 56          |\n",
            "|    time_elapsed         | 1106        |\n",
            "|    total_timesteps      | 114688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025124341 |\n",
            "|    clip_fraction        | 0.263       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.2       |\n",
            "|    explained_variance   | 0.663       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.9         |\n",
            "|    n_updates            | 550         |\n",
            "|    policy_gradient_loss | -0.014      |\n",
            "|    reward               | 1.3321081   |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 9.08        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 57          |\n",
            "|    time_elapsed         | 1126        |\n",
            "|    total_timesteps      | 116736      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.034382932 |\n",
            "|    clip_fraction        | 0.366       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.2       |\n",
            "|    explained_variance   | 0.379       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.8         |\n",
            "|    n_updates            | 560         |\n",
            "|    policy_gradient_loss | -0.00866    |\n",
            "|    reward               | -0.87873405 |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 14.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 58          |\n",
            "|    time_elapsed         | 1147        |\n",
            "|    total_timesteps      | 118784      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027445223 |\n",
            "|    clip_fraction        | 0.289       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.3       |\n",
            "|    explained_variance   | 0.714       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 4.25        |\n",
            "|    n_updates            | 570         |\n",
            "|    policy_gradient_loss | -0.0208     |\n",
            "|    reward               | 1.30668     |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 9.94        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 59          |\n",
            "|    time_elapsed         | 1166        |\n",
            "|    total_timesteps      | 120832      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029181672 |\n",
            "|    clip_fraction        | 0.249       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.3       |\n",
            "|    explained_variance   | 0.385       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.94        |\n",
            "|    n_updates            | 580         |\n",
            "|    policy_gradient_loss | -0.014      |\n",
            "|    reward               | -0.94925106 |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 20.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 60          |\n",
            "|    time_elapsed         | 1187        |\n",
            "|    total_timesteps      | 122880      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.02264988  |\n",
            "|    clip_fraction        | 0.255       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.4       |\n",
            "|    explained_variance   | 0.154       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.84        |\n",
            "|    n_updates            | 590         |\n",
            "|    policy_gradient_loss | -0.0131     |\n",
            "|    reward               | -0.07525982 |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 14.2        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 80\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 1165737.10\n",
            "total_reward: 165737.10\n",
            "total_cost: 2114404.67\n",
            "total_trades: 72062\n",
            "Sharpe: 0.168\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 61          |\n",
            "|    time_elapsed         | 1207        |\n",
            "|    total_timesteps      | 124928      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025851298 |\n",
            "|    clip_fraction        | 0.257       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.4       |\n",
            "|    explained_variance   | 0.38        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 10.8        |\n",
            "|    n_updates            | 600         |\n",
            "|    policy_gradient_loss | -0.0169     |\n",
            "|    reward               | 0.8298221   |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 20.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 62          |\n",
            "|    time_elapsed         | 1226        |\n",
            "|    total_timesteps      | 126976      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.051597062 |\n",
            "|    clip_fraction        | 0.333       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.4       |\n",
            "|    explained_variance   | 0.407       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 9.29        |\n",
            "|    n_updates            | 610         |\n",
            "|    policy_gradient_loss | -0.00222    |\n",
            "|    reward               | 4.757155    |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 17.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 63          |\n",
            "|    time_elapsed         | 1248        |\n",
            "|    total_timesteps      | 129024      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022950804 |\n",
            "|    clip_fraction        | 0.221       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.5       |\n",
            "|    explained_variance   | 0.269       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 9.17        |\n",
            "|    n_updates            | 620         |\n",
            "|    policy_gradient_loss | -0.00755    |\n",
            "|    reward               | 1.4871265   |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 21.6        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 103        |\n",
            "|    iterations           | 64         |\n",
            "|    time_elapsed         | 1268       |\n",
            "|    total_timesteps      | 131072     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02820847 |\n",
            "|    clip_fraction        | 0.27       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43.5      |\n",
            "|    explained_variance   | 0.307      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 9.04       |\n",
            "|    n_updates            | 630        |\n",
            "|    policy_gradient_loss | -0.0103    |\n",
            "|    reward               | -0.3275326 |\n",
            "|    std                  | 1.09       |\n",
            "|    value_loss           | 25.7       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 65          |\n",
            "|    time_elapsed         | 1287        |\n",
            "|    total_timesteps      | 133120      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019836836 |\n",
            "|    clip_fraction        | 0.193       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.6       |\n",
            "|    explained_variance   | 0.197       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 17.5        |\n",
            "|    n_updates            | 640         |\n",
            "|    policy_gradient_loss | -0.00682    |\n",
            "|    reward               | -1.1818279  |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 33.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 66          |\n",
            "|    time_elapsed         | 1307        |\n",
            "|    total_timesteps      | 135168      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.056084312 |\n",
            "|    clip_fraction        | 0.545       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.7       |\n",
            "|    explained_variance   | 0.413       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.28        |\n",
            "|    n_updates            | 650         |\n",
            "|    policy_gradient_loss | 0.0316      |\n",
            "|    reward               | 1.662835    |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 11.8        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 103        |\n",
            "|    iterations           | 67         |\n",
            "|    time_elapsed         | 1328       |\n",
            "|    total_timesteps      | 137216     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.09770877 |\n",
            "|    clip_fraction        | 0.515      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43.8      |\n",
            "|    explained_variance   | 0.238      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 7.71       |\n",
            "|    n_updates            | 660        |\n",
            "|    policy_gradient_loss | 0.0178     |\n",
            "|    reward               | -1.3936406 |\n",
            "|    std                  | 1.1        |\n",
            "|    value_loss           | 16.3       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 68          |\n",
            "|    time_elapsed         | 1347        |\n",
            "|    total_timesteps      | 139264      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013015756 |\n",
            "|    clip_fraction        | 0.112       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.9       |\n",
            "|    explained_variance   | 0.0763      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 52.3        |\n",
            "|    n_updates            | 670         |\n",
            "|    policy_gradient_loss | -0.00538    |\n",
            "|    reward               | 0.2962724   |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 85.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 69          |\n",
            "|    time_elapsed         | 1367        |\n",
            "|    total_timesteps      | 141312      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022751413 |\n",
            "|    clip_fraction        | 0.288       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.9       |\n",
            "|    explained_variance   | 0.0903      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 33.2        |\n",
            "|    n_updates            | 680         |\n",
            "|    policy_gradient_loss | 0.00545     |\n",
            "|    reward               | -1.0708741  |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 69          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 70          |\n",
            "|    time_elapsed         | 1387        |\n",
            "|    total_timesteps      | 143360      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022044864 |\n",
            "|    clip_fraction        | 0.252       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.9       |\n",
            "|    explained_variance   | 0.176       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.07        |\n",
            "|    n_updates            | 690         |\n",
            "|    policy_gradient_loss | -0.00703    |\n",
            "|    reward               | 0.23947975  |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 13.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 71          |\n",
            "|    time_elapsed         | 1407        |\n",
            "|    total_timesteps      | 145408      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.034712132 |\n",
            "|    clip_fraction        | 0.271       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44         |\n",
            "|    explained_variance   | 0.33        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 15.4        |\n",
            "|    n_updates            | 700         |\n",
            "|    policy_gradient_loss | -0.00702    |\n",
            "|    reward               | 0.34281787  |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 26.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 72          |\n",
            "|    time_elapsed         | 1427        |\n",
            "|    total_timesteps      | 147456      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017912755 |\n",
            "|    clip_fraction        | 0.161       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44         |\n",
            "|    explained_variance   | 0.301       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 18.7        |\n",
            "|    n_updates            | 710         |\n",
            "|    policy_gradient_loss | -0.0151     |\n",
            "|    reward               | -5.409659   |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 34.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 73          |\n",
            "|    time_elapsed         | 1446        |\n",
            "|    total_timesteps      | 149504      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028228762 |\n",
            "|    clip_fraction        | 0.261       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44         |\n",
            "|    explained_variance   | 0.601       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.68        |\n",
            "|    n_updates            | 720         |\n",
            "|    policy_gradient_loss | -0.00967    |\n",
            "|    reward               | -1.128434   |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 10.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 74          |\n",
            "|    time_elapsed         | 1466        |\n",
            "|    total_timesteps      | 151552      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028157596 |\n",
            "|    clip_fraction        | 0.241       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44         |\n",
            "|    explained_variance   | 0.288       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.2         |\n",
            "|    n_updates            | 730         |\n",
            "|    policy_gradient_loss | -0.0059     |\n",
            "|    reward               | -1.2263464  |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 22.2        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 90\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 2683073.71\n",
            "total_reward: 1683073.71\n",
            "total_cost: 1874430.73\n",
            "total_trades: 70518\n",
            "Sharpe: 0.545\n",
            "=================================\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 103        |\n",
            "|    iterations           | 75         |\n",
            "|    time_elapsed         | 1486       |\n",
            "|    total_timesteps      | 153600     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02852372 |\n",
            "|    clip_fraction        | 0.28       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.1      |\n",
            "|    explained_variance   | 0.133      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 39.9       |\n",
            "|    n_updates            | 740        |\n",
            "|    policy_gradient_loss | -0.00218   |\n",
            "|    reward               | 1.2674067  |\n",
            "|    std                  | 1.11       |\n",
            "|    value_loss           | 60.8       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 76          |\n",
            "|    time_elapsed         | 1505        |\n",
            "|    total_timesteps      | 155648      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018645441 |\n",
            "|    clip_fraction        | 0.151       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.1       |\n",
            "|    explained_variance   | 0.248       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 16.9        |\n",
            "|    n_updates            | 750         |\n",
            "|    policy_gradient_loss | -0.00807    |\n",
            "|    reward               | -4.0920496  |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 39.8        |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 103       |\n",
            "|    iterations           | 77        |\n",
            "|    time_elapsed         | 1525      |\n",
            "|    total_timesteps      | 157696    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0270642 |\n",
            "|    clip_fraction        | 0.275     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -44.1     |\n",
            "|    explained_variance   | 0.155     |\n",
            "|    learning_rate        | 0.00025   |\n",
            "|    loss                 | 11.7      |\n",
            "|    n_updates            | 760       |\n",
            "|    policy_gradient_loss | -0.00524  |\n",
            "|    reward               | 2.2659616 |\n",
            "|    std                  | 1.11      |\n",
            "|    value_loss           | 25.6      |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 78          |\n",
            "|    time_elapsed         | 1544        |\n",
            "|    total_timesteps      | 159744      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023003783 |\n",
            "|    clip_fraction        | 0.231       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.2       |\n",
            "|    explained_variance   | 0.0989      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 46.2        |\n",
            "|    n_updates            | 770         |\n",
            "|    policy_gradient_loss | -0.00615    |\n",
            "|    reward               | 2.450436    |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 70.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 79          |\n",
            "|    time_elapsed         | 1564        |\n",
            "|    total_timesteps      | 161792      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.05317783  |\n",
            "|    clip_fraction        | 0.413       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.2       |\n",
            "|    explained_variance   | 0.0634      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 38.4        |\n",
            "|    n_updates            | 780         |\n",
            "|    policy_gradient_loss | 0.0117      |\n",
            "|    reward               | -10.6462755 |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 103         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 80          |\n",
            "|    time_elapsed         | 1584        |\n",
            "|    total_timesteps      | 163840      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028667813 |\n",
            "|    clip_fraction        | 0.249       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.2       |\n",
            "|    explained_variance   | 0.118       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 26.9        |\n",
            "|    n_updates            | 790         |\n",
            "|    policy_gradient_loss | -0.00324    |\n",
            "|    reward               | 0.26653972  |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 62.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 81          |\n",
            "|    time_elapsed         | 1603        |\n",
            "|    total_timesteps      | 165888      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031772442 |\n",
            "|    clip_fraction        | 0.201       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.3       |\n",
            "|    explained_variance   | 0.106       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 27.1        |\n",
            "|    n_updates            | 800         |\n",
            "|    policy_gradient_loss | -0.00703    |\n",
            "|    reward               | -0.5348155  |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 74.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 82          |\n",
            "|    time_elapsed         | 1623        |\n",
            "|    total_timesteps      | 167936      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022636864 |\n",
            "|    clip_fraction        | 0.196       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.3       |\n",
            "|    explained_variance   | 0.212       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 52.1        |\n",
            "|    n_updates            | 810         |\n",
            "|    policy_gradient_loss | -0.00538    |\n",
            "|    reward               | -0.23845893 |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 76.6        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 103        |\n",
            "|    iterations           | 83         |\n",
            "|    time_elapsed         | 1642       |\n",
            "|    total_timesteps      | 169984     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01715378 |\n",
            "|    clip_fraction        | 0.089      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.3      |\n",
            "|    explained_variance   | 0.217      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 75.6       |\n",
            "|    n_updates            | 820        |\n",
            "|    policy_gradient_loss | -0.00612   |\n",
            "|    reward               | -1.4941722 |\n",
            "|    std                  | 1.12       |\n",
            "|    value_loss           | 95         |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 103        |\n",
            "|    iterations           | 84         |\n",
            "|    time_elapsed         | 1662       |\n",
            "|    total_timesteps      | 172032     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02511689 |\n",
            "|    clip_fraction        | 0.212      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.4      |\n",
            "|    explained_variance   | 0.121      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 10.5       |\n",
            "|    n_updates            | 830        |\n",
            "|    policy_gradient_loss | -0.00666   |\n",
            "|    reward               | -2.6143408 |\n",
            "|    std                  | 1.12       |\n",
            "|    value_loss           | 25.3       |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 103          |\n",
            "|    iterations           | 85           |\n",
            "|    time_elapsed         | 1681         |\n",
            "|    total_timesteps      | 174080       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.014418612  |\n",
            "|    clip_fraction        | 0.173        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -44.4        |\n",
            "|    explained_variance   | 0.1          |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 41.3         |\n",
            "|    n_updates            | 840          |\n",
            "|    policy_gradient_loss | -0.00111     |\n",
            "|    reward               | -0.061271872 |\n",
            "|    std                  | 1.12         |\n",
            "|    value_loss           | 128          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 86          |\n",
            "|    time_elapsed         | 1700        |\n",
            "|    total_timesteps      | 176128      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.041056722 |\n",
            "|    clip_fraction        | 0.248       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.4       |\n",
            "|    explained_variance   | 0.162       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 46.6        |\n",
            "|    n_updates            | 850         |\n",
            "|    policy_gradient_loss | 0.00252     |\n",
            "|    reward               | 0.33376035  |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 135         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 87          |\n",
            "|    time_elapsed         | 1720        |\n",
            "|    total_timesteps      | 178176      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028127715 |\n",
            "|    clip_fraction        | 0.271       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.5       |\n",
            "|    explained_variance   | 0.253       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 22.2        |\n",
            "|    n_updates            | 860         |\n",
            "|    policy_gradient_loss | -0.00289    |\n",
            "|    reward               | -0.39373142 |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 41.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 88          |\n",
            "|    time_elapsed         | 1740        |\n",
            "|    total_timesteps      | 180224      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021236308 |\n",
            "|    clip_fraction        | 0.259       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.5       |\n",
            "|    explained_variance   | 0.141       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 22.7        |\n",
            "|    n_updates            | 870         |\n",
            "|    policy_gradient_loss | -0.00169    |\n",
            "|    reward               | -1.7168705  |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 70.9        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 100\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3138103.79\n",
            "total_reward: 2138103.79\n",
            "total_cost: 1394449.81\n",
            "total_trades: 65815\n",
            "Sharpe: 0.588\n",
            "=================================\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 103          |\n",
            "|    iterations           | 89           |\n",
            "|    time_elapsed         | 1759         |\n",
            "|    total_timesteps      | 182272       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0196806    |\n",
            "|    clip_fraction        | 0.198        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -44.5        |\n",
            "|    explained_variance   | 0.142        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 24           |\n",
            "|    n_updates            | 880          |\n",
            "|    policy_gradient_loss | -0.00326     |\n",
            "|    reward               | -0.010154683 |\n",
            "|    std                  | 1.12         |\n",
            "|    value_loss           | 49.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 103          |\n",
            "|    iterations           | 90           |\n",
            "|    time_elapsed         | 1779         |\n",
            "|    total_timesteps      | 184320       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.058465824  |\n",
            "|    clip_fraction        | 0.343        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -44.5        |\n",
            "|    explained_variance   | 0.22         |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 54.8         |\n",
            "|    n_updates            | 890          |\n",
            "|    policy_gradient_loss | 0.00693      |\n",
            "|    reward               | -0.021737503 |\n",
            "|    std                  | 1.13         |\n",
            "|    value_loss           | 70.8         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 91          |\n",
            "|    time_elapsed         | 1798        |\n",
            "|    total_timesteps      | 186368      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.040312808 |\n",
            "|    clip_fraction        | 0.365       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.6       |\n",
            "|    explained_variance   | 0.108       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 10.8        |\n",
            "|    n_updates            | 900         |\n",
            "|    policy_gradient_loss | -0.00393    |\n",
            "|    reward               | -0.46774077 |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 22.3        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 103        |\n",
            "|    iterations           | 92         |\n",
            "|    time_elapsed         | 1818       |\n",
            "|    total_timesteps      | 188416     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.07189278 |\n",
            "|    clip_fraction        | 0.359      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.7      |\n",
            "|    explained_variance   | 0.104      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 14.5       |\n",
            "|    n_updates            | 910        |\n",
            "|    policy_gradient_loss | 0.00971    |\n",
            "|    reward               | -2.0027587 |\n",
            "|    std                  | 1.13       |\n",
            "|    value_loss           | 36.4       |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 103        |\n",
            "|    iterations           | 93         |\n",
            "|    time_elapsed         | 1838       |\n",
            "|    total_timesteps      | 190464     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02197975 |\n",
            "|    clip_fraction        | 0.222      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.7      |\n",
            "|    explained_variance   | 0.258      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 19.6       |\n",
            "|    n_updates            | 920        |\n",
            "|    policy_gradient_loss | 0.00554    |\n",
            "|    reward               | -1.4731233 |\n",
            "|    std                  | 1.13       |\n",
            "|    value_loss           | 48.9       |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 103        |\n",
            "|    iterations           | 94         |\n",
            "|    time_elapsed         | 1857       |\n",
            "|    total_timesteps      | 192512     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04889624 |\n",
            "|    clip_fraction        | 0.412      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.8      |\n",
            "|    explained_variance   | 0.103      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 16.5       |\n",
            "|    n_updates            | 930        |\n",
            "|    policy_gradient_loss | 0.018      |\n",
            "|    reward               | -1.6655031 |\n",
            "|    std                  | 1.14       |\n",
            "|    value_loss           | 31         |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 95          |\n",
            "|    time_elapsed         | 1877        |\n",
            "|    total_timesteps      | 194560      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.041178018 |\n",
            "|    clip_fraction        | 0.364       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.9       |\n",
            "|    explained_variance   | 0.0746      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 24.6        |\n",
            "|    n_updates            | 940         |\n",
            "|    policy_gradient_loss | -0.000368   |\n",
            "|    reward               | -0.89929    |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 47.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 96          |\n",
            "|    time_elapsed         | 1896        |\n",
            "|    total_timesteps      | 196608      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017282907 |\n",
            "|    clip_fraction        | 0.227       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45         |\n",
            "|    explained_variance   | 0.162       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 27.3        |\n",
            "|    n_updates            | 950         |\n",
            "|    policy_gradient_loss | -0.000362   |\n",
            "|    reward               | 4.5539756   |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 65.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 97          |\n",
            "|    time_elapsed         | 1916        |\n",
            "|    total_timesteps      | 198656      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029597625 |\n",
            "|    clip_fraction        | 0.335       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45         |\n",
            "|    explained_variance   | 0.0807      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 17.3        |\n",
            "|    n_updates            | 960         |\n",
            "|    policy_gradient_loss | 0.00107     |\n",
            "|    reward               | -1.3615372  |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 44.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 103         |\n",
            "|    iterations           | 98          |\n",
            "|    time_elapsed         | 1936        |\n",
            "|    total_timesteps      | 200704      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.06425942  |\n",
            "|    clip_fraction        | 0.326       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45         |\n",
            "|    explained_variance   | 0.129       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 20.8        |\n",
            "|    n_updates            | 970         |\n",
            "|    policy_gradient_loss | 0.00244     |\n",
            "|    reward               | -0.52521914 |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 60.6        |\n",
            "-----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_ppo = agent.train_model(model=model_ppo,\n",
        "                             tb_log_name='ppo',\n",
        "                             total_timesteps=200000) if if_using_ppo else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "C6AidlWyvwzm"
      },
      "outputs": [],
      "source": [
        "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zpv4S0-fDBv"
      },
      "source": [
        "### Agent 4: TD3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "JSAHhV4Xc-bh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b4ea069-67d9-4af9-fea4-79b017963a4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cuda device\n",
            "Logging to results/td3\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "TD3_PARAMS = {\"batch_size\": 100,\n",
        "              \"buffer_size\": 1000000,\n",
        "              \"learning_rate\": 0.001}\n",
        "\n",
        "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
        "\n",
        "if if_using_td3:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/td3'\n",
        "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_td3.set_logger(new_logger_td3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OSRxNYAxdKpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4716c8fe-958f-4255-c149-13fbdb247145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "day: 2892, episode: 110\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5616463.37\n",
            "total_reward: 4616463.37\n",
            "total_cost: 11680.19\n",
            "total_trades: 26154\n",
            "Sharpe: 0.908\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 69        |\n",
            "|    time_elapsed    | 165       |\n",
            "|    total_timesteps | 11572     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 108       |\n",
            "|    critic_loss     | 181       |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 11471     |\n",
            "|    reward          | 5.7246976 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 69        |\n",
            "|    time_elapsed    | 332       |\n",
            "|    total_timesteps | 23144     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 73.8      |\n",
            "|    critic_loss     | 41.4      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 23043     |\n",
            "|    reward          | 5.7246976 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 69        |\n",
            "|    time_elapsed    | 499       |\n",
            "|    total_timesteps | 34716     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 44.4      |\n",
            "|    critic_loss     | 195       |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 34615     |\n",
            "|    reward          | 5.7246976 |\n",
            "----------------------------------\n",
            "day: 2892, episode: 120\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5616463.37\n",
            "total_reward: 4616463.37\n",
            "total_cost: 11680.19\n",
            "total_trades: 26154\n",
            "Sharpe: 0.908\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 69        |\n",
            "|    time_elapsed    | 665       |\n",
            "|    total_timesteps | 46288     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 36.6      |\n",
            "|    critic_loss     | 22.1      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 46187     |\n",
            "|    reward          | 5.7246976 |\n",
            "----------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_td3 = agent.train_model(model=model_td3,\n",
        "                             tb_log_name='td3',\n",
        "                             total_timesteps=50000) if if_using_td3 else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "OkJV6V_mv2hw"
      },
      "outputs": [],
      "source": [
        "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr49PotrfG01"
      },
      "source": [
        "### Agent 5: SAC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xwOhVjqRkCdM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4ff1a4c-3d59-433c-a82a-43348e8b9f71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cuda device\n",
            "Logging to results/sac\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "SAC_PARAMS = {\n",
        "    \"batch_size\": 128,\n",
        "    \"buffer_size\": 100000,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"learning_starts\": 100,\n",
        "    \"ent_coef\": \"auto_0.1\",\n",
        "}\n",
        "\n",
        "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
        "\n",
        "if if_using_sac:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/sac'\n",
        "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_sac.set_logger(new_logger_sac)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8RSdKCckJyH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2191865c-9a63-495a-f445-c8ce287d36e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 49       |\n",
            "|    time_elapsed    | 232      |\n",
            "|    total_timesteps | 11572    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 679      |\n",
            "|    critic_loss     | 101      |\n",
            "|    ent_coef        | 0.159    |\n",
            "|    ent_coef_loss   | -85.1    |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 11471    |\n",
            "|    reward          | 3.089759 |\n",
            "---------------------------------\n",
            "day: 2892, episode: 130\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 6474524.88\n",
            "total_reward: 5474524.88\n",
            "total_cost: 1107649.16\n",
            "total_trades: 66262\n",
            "Sharpe: 0.894\n",
            "=================================\n"
          ]
        }
      ],
      "source": [
        "trained_sac = agent.train_model(model=model_sac,\n",
        "                             tb_log_name='sac',\n",
        "                             total_timesteps=70000) if if_using_sac else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SpZoQgPv7GO"
      },
      "outputs": [],
      "source": [
        "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgGm3dQZfRks"
      },
      "source": [
        "## Save the trained agent\n",
        "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
        "\n",
        "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
        "\n",
        "For users running on your local environment, the zip files should be at \"./trained_models\"."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "MRiOtrywfAo1",
        "_gDkU-j-fCmZ",
        "3Zpv4S0-fDBv",
        "Dr49PotrfG01"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}